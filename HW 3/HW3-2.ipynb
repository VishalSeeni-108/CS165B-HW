{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f8r6KvzYF38"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL7n0V-ChdkU"
   },
   "source": [
    "## MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cDAuBwgUmVzI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import transformers\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "# you may also import other modules if you need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vyAuHIaLmWDi"
   },
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "hdH82edDmb1R"
   },
   "outputs": [],
   "source": [
    "# Define the CNN model with the specified architecture\n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomMLP, self).__init__()\n",
    "\n",
    "        # Build your MLP model by filling into nn.Sequential() or writing your own layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            # Flatten layer\n",
    "            nn.Flatten(),\n",
    "            # Fully connected layer with 256 units, ReLU activation. You need to calculate the input dimension of this layer.\n",
    "            nn.Linear(784,256),\n",
    "            nn.ReLU(),\n",
    "            # Fully connected layer with 128 units, ReLU activation.\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            # Dropout layer: dropout rate of 0.5\n",
    "            nn.Dropout(p=0.5),\n",
    "            # Output layer with softmax activation (10 classes)\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ydg_jXlamcf5"
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "BATCH_SIZE = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Build train dataset and dataloader\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Build testing dataset and dataloader\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "i3P-4eBWmlwF"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the CNN model\n",
    "MLP_model = CustomMLP(num_classes=10).to(device)\n",
    "\n",
    "# Define loss function and optimizer for CNN model\n",
    "optimizer = optim.Adam(MLP_model.parameters(), lr=learning_rate)\n",
    "# Write down the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFuasWxszNlA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 [0/60000 (0%)]\tLoss: 2.304106\t Accuracy:7.812%\n",
      "Epoch : 1 [6400/60000 (11%)]\tLoss: 2.307061\t Accuracy:9.375%\n",
      "Epoch : 1 [12800/60000 (21%)]\tLoss: 2.318873\t Accuracy:9.313%\n",
      "Epoch : 1 [19200/60000 (32%)]\tLoss: 2.309386\t Accuracy:9.219%\n",
      "Epoch : 1 [25600/60000 (43%)]\tLoss: 2.313713\t Accuracy:9.336%\n",
      "Epoch : 1 [32000/60000 (53%)]\tLoss: 2.294111\t Accuracy:9.328%\n",
      "Epoch : 1 [38400/60000 (64%)]\tLoss: 2.299706\t Accuracy:9.162%\n",
      "Epoch : 1 [44800/60000 (75%)]\tLoss: 2.313535\t Accuracy:9.074%\n",
      "Epoch : 1 [51200/60000 (85%)]\tLoss: 2.305896\t Accuracy:8.971%\n",
      "Epoch : 1 [57600/60000 (96%)]\tLoss: 2.294707\t Accuracy:8.964%\n",
      "Epoch [1/10], MLP Train Loss: 2.3054, MLP Train Acc: 0.0896\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to list.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 67\u001b[0m\n\u001b[0;32m     63\u001b[0m     test_acc\u001b[38;5;241m.\u001b[39mappend(test_epoch_acc)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# define any variables you may need to calculate per-class accuracy\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Testing Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Calculate per-class precision, recall, and F1\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to list.__format__"
     ]
    }
   ],
   "source": [
    "# Lists to store training metrics\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "# Training loop for CNN model\n",
    "for epoch in range(num_epochs):\n",
    "    MLP_model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    class_corrects = np.zeros(10)\n",
    "    class_totals = np.zeros(10)\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Training process here, which includes forward pass of the model, backpropogate of the loss, etc.\n",
    "        # Remember to use the optimizer and criterion defined previously.\n",
    "\n",
    "        outputs = MLP_model(images.view(images.size(0), -1))\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                epoch+1, batch_idx*len(images), len(train_loader.dataset), 100.*batch_idx /\n",
    "                len(train_loader), loss.item(), float(running_corrects*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_acc.append(epoch_acc)\n",
    "\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], MLP Train Loss: {epoch_loss:.4f}, MLP Train Acc: {epoch_acc:.4f}')\n",
    "\n",
    "    # Testing loop for MLP model\n",
    "    MLP_model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    test_running_corrects = 0\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Testing process\n",
    "        # You may need to calculate test_running_loss and test_running_corrects, in order to get this epoch's Testing loss and accuracy\n",
    "        outputs = MLP_model(images.view(images.size(0), -1))\n",
    "        loss = criterion(outputs, labels)\n",
    "        preds = torch.max(outputs, 1)[1]\n",
    "        test_running_loss += loss.item() * images.size(0)\n",
    "        test_running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    test_epoch_loss = test_running_loss / len(test_loader.dataset)\n",
    "    test_epoch_acc = test_running_corrects.double() / len(test_loader.dataset)\n",
    "\n",
    "    test_loss.append(test_epoch_loss)\n",
    "    test_acc.append(test_epoch_acc)\n",
    "\n",
    "    # define any variables you may need to calculate per-class accuracy\n",
    "\n",
    "    print(f'Testing Loss: {test_epoch_loss:.4f}, Testing Acc: {test_epoch_acc:.4f}')\n",
    "    print()\n",
    "\n",
    "# Calculate per-class precision, recall, and F1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1OYe93fhp44"
   },
   "source": [
    "## Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMnApXCSzIVW"
   },
   "outputs": [],
   "source": [
    "# The Vision Transformers are designed for images with multiple color channels,\n",
    "# and MNIST images are grayscale with only one channel.\n",
    "# To address this, we need to modify the preprocessing accordingly.\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),  # Resize images to ViT input size\n",
    "    torchvision.transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel grayscale\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4y5vK8a5XVY"
   },
   "outputs": [],
   "source": [
    "# Build the ViT model\n",
    "vit_model =\n",
    "# You need to modify the classification head to match the number of classes in MNIST\n",
    "class_names = [str(i) for i in range(10)]\n",
    "vit_model.classifier ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doBQdUU4cH1V"
   },
   "outputs": [],
   "source": [
    "# Freeze the backbone (ViT) weights\n",
    "for param in vit_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the classification head weights\n",
    "for param in vit_model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRzDlvBicQt5"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vit_model.to(device)\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 5\n",
    "\n",
    "# Define optimizer and criterion for ViT model\n",
    "vit_optimizer = torch.optim.AdamW(vit_model.classifier.parameters(), lr=learning_rate)\n",
    "vit_criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mp53kQJq24gP"
   },
   "outputs": [],
   "source": [
    "# Lists to store training and testing metrics for attention-based model\n",
    "attention_train_loss = []\n",
    "attention_train_acc = []\n",
    "attention_test_loss = []\n",
    "attention_test_acc = []\n",
    "attention_per_class_acc = np.zeros(10)\n",
    "\n",
    "# Training loop for Attention model\n",
    "for epoch in range(num_epochs):\n",
    "    vit_model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    class_corrects = np.zeros(10)\n",
    "    class_totals = np.zeros(10)\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Training process here\n",
    "        # Similar to MLP model, we also need to implement the forward pass, backpropogation, etc.\n",
    "        outputs =\n",
    "        loss =\n",
    "\n",
    "        # calculate the number of correctly classified data and loss for this batch\n",
    "        running_corrects +=\n",
    "        running_loss +=\n",
    "\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                epoch+1, batch_idx*len(inputs), len(train_loader.dataset), 100.*batch_idx /\n",
    "                len(train_loader), loss.item(), float(running_corrects*100) / float(BATCH_SIZE*(batch_idx+1))))\n",
    "\n",
    "    attention_epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    attention_epoch_acc = running_corrects / len(train_loader.dataset)\n",
    "\n",
    "    attention_train_loss.append(attention_epoch_loss)\n",
    "    attention_train_acc.append(attention_epoch_acc)\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Attention Model Train Loss: {attention_epoch_loss:.4f}, Attention Train Acc: {attention_epoch_acc:.4f}\")\n",
    "\n",
    "    # Testing loop for ViT model\n",
    "    vit_model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    test_running_corrects = 0\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Testing process\n",
    "        # You may need to calculate test_running_loss and test_running_corrects, in order to get this epoch's testing loss and accuracy\n",
    "\n",
    "        outputs =\n",
    "        loss =\n",
    "\n",
    "        test_running_loss +=\n",
    "        test_running_corrects +=\n",
    "\n",
    "    attention_test_epoch_loss = test_running_loss / len(test_loader.dataset)\n",
    "    attention_test_epoch_acc = test_running_corrects / len(test_loader.dataset)\n",
    "\n",
    "    attention_test_loss.append(attention_test_epoch_loss)\n",
    "    attention_test_acc.append(attention_test_epoch_acc)\n",
    "\n",
    "    # define any variables you may need to calculate per-class accuracy\n",
    "\n",
    "    print(f'Attention Model Test Loss: {attention_test_epoch_loss:.4f}, Attention Model Test Acc: {attention_test_epoch_acc:.4f}')\n",
    "    print()\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save the trained model\n",
    "vit_model.save_pretrained(\"mnist_vit_model\")\n",
    "# Calculate per-class accuracy\n",
    "attention_per_class_acc =\n",
    "print('Attention Per-Class Accuracy For the Best Model:', attention_per_class_acc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
